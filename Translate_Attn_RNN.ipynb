{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation with Attention RNN (PyTorch)\n",
    "\n",
    "Notebook ini mereplikasi pipeline **machine translation** sederhana seperti yang kamu bangun di Rust (tch-rs), namun dalam **PyTorch (Python)**:\n",
    "\n",
    "1. **Data loader**: baca CSV dua kolom `(source, target)`.\n",
    "2. **Dual tokenizer** (source & target, word-level sederhana) + special tokens `PAD=0, SOS=1, EOS=2, UNK=3`.\n",
    "3. **Model**: Encoder bi-LSTM + Decoder LSTM + **Bahdanau Attention** + output projection ke vocab **TARGET**.\n",
    "4. **Training** dengan teacher forcing, **CrossEntropy(ignore_index=PAD)**.\n",
    "5. **Evaluasi**: mini BLEU & ROUGE-1, dan **greedy decoding** untuk contoh.\n",
    "\n",
    "> Catatan: Notebook ini **tidak butuh internet**. Pastikan `torch`, `pandas`, `numpy` tersedia di environment kamu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 0) Setup & Utilities\n",
    "import math, os, random, time, gc\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Pandas diperlukan untuk membaca CSV. Install dengan: pip install pandas\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"✓ Device: {device}\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Memakai CSV: Translate500.csv\n",
      "                    source                    target\n",
      "0              shop, store               toko, kedai\n",
      "1                    price                     harga\n",
      "2  to be near, to be close                     dekat\n",
      "3  sometimes, occasionally  kadang-kadang, terkadang\n",
      "4           to be possible              memungkinkan\n",
      "Total pairs: 492\n",
      "Train: 442 | Val: 50\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 1) Load Data (CSV dua kolom: source, target)\n",
    "from pathlib import Path\n",
    "\n",
    "def robust_read_csv(path):\n",
    "    \"\"\"Robust CSV reader: coba beberapa opsi agar tidak error saat parsing.\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return pd.read_csv(path, engine='python')\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return pd.read_csv(path, on_bad_lines='skip', engine='python')\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "CSV_CANDIDATES = [\n",
    "    'Translate500.csv',\n",
    "    '../Translate500.csv',\n",
    "]\n",
    "\n",
    "df = None\n",
    "picked = None\n",
    "for p in CSV_CANDIDATES:\n",
    "    if Path(p).exists():\n",
    "        try:\n",
    "            tmp = robust_read_csv(p)\n",
    "            if tmp.shape[1] >= 2:\n",
    "                df = tmp\n",
    "                picked = p\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Gagal baca {p}: {e}\")\n",
    "\n",
    "if df is None:\n",
    "    print(\"CSV tidak ditemukan. Membuat dataset kecil mainan...\")\n",
    "    df = pd.DataFrame({\n",
    "        'source': [\n",
    "            'hello how are you',\n",
    "            'what is your name',\n",
    "            'good morning',\n",
    "            'see you later',\n",
    "            'thank you very much',\n",
    "            'i love machine learning',\n",
    "            'this model uses attention',\n",
    "        ],\n",
    "        'target': [\n",
    "            'halo apa kabar',\n",
    "            'siapa namamu',\n",
    "            'selamat pagi',\n",
    "            'sampai jumpa',\n",
    "            'terima kasih banyak',\n",
    "            'aku suka pembelajaran mesin',\n",
    "            'model ini memakai atensi',\n",
    "        ]\n",
    "    })\n",
    "else:\n",
    "    print(f\"✓ Memakai CSV: {picked}\")\n",
    "\n",
    "# Normalisasi kolom dan cleaning ringan\n",
    "if 'source' not in df.columns or 'target' not in df.columns:\n",
    "    # Ambil dua kolom pertama jika header tidak pas\n",
    "    df = df.iloc[:, :2].copy()\n",
    "    df.columns = ['source', 'target']\n",
    "df = df[['source', 'target']].dropna()\n",
    "df['source'] = df['source'].astype(str).str.strip()\n",
    "df['target'] = df['target'].astype(str).str.strip()\n",
    "df = df[(df['source']!='') & (df['target']!='')]\n",
    "df = df.drop_duplicates()\n",
    "print(df.head())\n",
    "print(\"Total pairs:\", len(df))\n",
    "\n",
    "# Train/Val split\n",
    "ratio = 0.9\n",
    "idx = list(range(len(df)))\n",
    "random.shuffle(idx)\n",
    "cut = int(len(idx)*ratio)\n",
    "train_df = df.iloc[idx[:cut]].reset_index(drop=True)\n",
    "val_df   = df.iloc[idx[cut:]].reset_index(drop=True)\n",
    "print(f\"Train: {len(train_df)} | Val: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Vocab: 662 | Target Vocab: 599\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 2) Dual Tokenizer (word-level sederhana)\n",
    "PAD, SOS, EOS, UNK = 0, 1, 2, 3\n",
    "\n",
    "class SimpleWordTokenizer:\n",
    "    def __init__(self, max_vocab=10000):\n",
    "        self.max_vocab = max_vocab\n",
    "        self.stoi = {\"<pad>\":PAD, \"<sos>\":SOS, \"<eos>\":EOS, \"<unk>\":UNK}\n",
    "        self.itos = {v:k for k,v in self.stoi.items()}\n",
    "\n",
    "    def fit(self, texts: List[str]):\n",
    "        counter = Counter()\n",
    "        for t in texts:\n",
    "            counter.update(t.split())\n",
    "        # sisakan slot untuk 4 special tokens\n",
    "        for tok, _ in counter.most_common(self.max_vocab - len(self.stoi)):\n",
    "            if tok not in self.stoi:\n",
    "                idx = len(self.stoi)\n",
    "                self.stoi[tok] = idx\n",
    "                self.itos[idx] = tok\n",
    "\n",
    "    def vocab_size(self):\n",
    "        return len(self.stoi)\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        return [self.stoi.get(tok, UNK) for tok in text.split()]\n",
    "\n",
    "    def encode_with_special(self, text: str, max_len: int, add_sos=False, add_eos=False) -> List[int]:\n",
    "        ids = self.encode(text)\n",
    "        if add_sos:\n",
    "            ids = [SOS] + ids\n",
    "        if add_eos:\n",
    "            ids = ids + [EOS]\n",
    "        # pad/trunc\n",
    "        ids = ids[:max_len]\n",
    "        ids = ids + [PAD] * (max_len - len(ids))\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        toks = []\n",
    "        for i in ids:\n",
    "            if i == EOS:\n",
    "                break\n",
    "            if i in (PAD, SOS):\n",
    "                continue\n",
    "            toks.append(self.itos.get(int(i), '<unk>'))\n",
    "        return ' '.join(toks)\n",
    "\n",
    "class DualTokenizer:\n",
    "    def __init__(self, src_max_vocab=10000, tgt_max_vocab=10000):\n",
    "        self.source = SimpleWordTokenizer(src_max_vocab)\n",
    "        self.target = SimpleWordTokenizer(tgt_max_vocab)\n",
    "\n",
    "    def fit(self, src_texts: List[str], tgt_texts: List[str]):\n",
    "        self.source.fit(src_texts)\n",
    "        self.target.fit(tgt_texts)\n",
    "\n",
    "dual_tok = DualTokenizer(10000, 10000)\n",
    "dual_tok.fit(train_df['source'].tolist(), train_df['target'].tolist())\n",
    "srcV = dual_tok.source.vocab_size()\n",
    "tgtV = dual_tok.target.vocab_size()\n",
    "print(f\"Source Vocab: {srcV} | Target Vocab: {tgtV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 3) Dataset & DataLoader\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, df, dual_tok: DualTokenizer, max_src_len=64, max_tgt_len=64):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tok = dual_tok\n",
    "        self.max_src_len = max_src_len\n",
    "        self.max_tgt_len = max_tgt_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = str(self.df.loc[idx, 'source'])\n",
    "        tgt = str(self.df.loc[idx, 'target'])\n",
    "\n",
    "        src_ids = self.tok.source.encode_with_special(src, self.max_src_len, add_sos=False, add_eos=True)\n",
    "        tgt_in  = self.tok.target.encode_with_special(tgt, self.max_tgt_len, add_sos=True,  add_eos=False)\n",
    "        tgt_out = self.tok.target.encode_with_special(tgt, self.max_tgt_len, add_sos=False, add_eos=True)\n",
    "\n",
    "        # mask 1/0: 1 untuk token selain PAD\n",
    "        src_mask = [1 if t != PAD else 0 for t in src_ids]\n",
    "\n",
    "        return (\n",
    "            torch.tensor(src_ids, dtype=torch.long),\n",
    "            torch.tensor(tgt_in,  dtype=torch.long),\n",
    "            torch.tensor(tgt_out, dtype=torch.long),\n",
    "            torch.tensor(src_mask, dtype=torch.bool),\n",
    "        )\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MAX_SRC_LEN = 64\n",
    "MAX_TGT_LEN = 64\n",
    "\n",
    "train_ds = TranslationDataset(train_df, dual_tok, MAX_SRC_LEN, MAX_TGT_LEN)\n",
    "val_ds   = TranslationDataset(val_df,   dual_tok, MAX_SRC_LEN, MAX_TGT_LEN)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "len(train_dl), len(val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18481495"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 4) Bahdanau Attention & Model (Encoder bi-LSTM, Decoder LSTM + Attention)\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim2):\n",
    "        super().__init__()\n",
    "        self.wq = nn.Linear(hidden_dim2, hidden_dim2, bias=False)\n",
    "        self.wk = nn.Linear(hidden_dim2, hidden_dim2, bias=False)\n",
    "        self.v  = nn.Linear(hidden_dim2, 1, bias=False)\n",
    "\n",
    "    def forward(self, query, keys, mask=None):\n",
    "        # query: [B, 2H], keys: [B, T, 2H], mask: [B, T]\n",
    "        q = self.wq(query).unsqueeze(1)          # [B,1,2H]\n",
    "        k = self.wk(keys)                        # [B,T,2H]\n",
    "        e = self.v(torch.tanh(q + k)).squeeze(-1) # [B,T]\n",
    "        if mask is not None:\n",
    "            m = mask.float()\n",
    "            e = e * m + (1.0 - m) * (-1e30)\n",
    "        attn = torch.softmax(e, dim=-1)          # [B,T]\n",
    "        context = torch.bmm(attn.unsqueeze(1), keys).squeeze(1)  # [B,2H]\n",
    "        return context, attn\n",
    "\n",
    "class AttentionRNN(nn.Module):\n",
    "    def __init__(self, src_vocab, tgt_vocab, emb_dim=256, hidden_dim=512, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.src_emb = nn.Embedding(src_vocab, emb_dim, padding_idx=PAD)\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab, emb_dim, padding_idx=PAD)\n",
    "\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        # decoder input: [emb_dim + 2*hidden_dim]\n",
    "        self.decoder = nn.LSTM(emb_dim + 2*hidden_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.attn = BahdanauAttention(hidden_dim * 2)\n",
    "        self.query_proj = nn.Linear(hidden_dim, hidden_dim * 2, bias=False)\n",
    "        self.out = nn.Linear(hidden_dim, tgt_vocab)\n",
    "\n",
    "    def encode(self, src_ids):\n",
    "        # src_ids: [B,T]\n",
    "        x = self.src_emb(src_ids)                     # [B,T,E]\n",
    "        enc_out, (h, c) = self.encoder(x)             # enc_out: [B,T,2H], h: [2L,B,H]\n",
    "        # init decoder state nol (sesuai implementasi Rust)\n",
    "        B = src_ids.size(0)\n",
    "        L = self.decoder.num_layers\n",
    "        H = self.decoder.hidden_size\n",
    "        device = src_ids.device\n",
    "        h0 = torch.zeros(L, B, H, device=device)\n",
    "        c0 = torch.zeros(L, B, H, device=device)\n",
    "        return enc_out, (h0, c0)\n",
    "\n",
    "    def decode_step(self, y_prev, state, enc_out, mask):\n",
    "        # y_prev: [B,1], state: (h,c) [L,B,H], enc_out: [B,T,2H], mask: [B,T]\n",
    "        emb = self.tgt_emb(y_prev)                    # [B,1,E]\n",
    "        h, c = state                                  # h: [L,B,H]\n",
    "        query = h[-1]                                 # [B,H]\n",
    "        q2 = self.query_proj(query)                   # [B,2H]\n",
    "        context, attn = self.attn(q2, enc_out, mask)  # [B,2H], [B,T]\n",
    "        dec_in = torch.cat([emb, context.unsqueeze(1)], dim=-1)  # [B,1,E+2H]\n",
    "        out, (h2, c2) = self.decoder(dec_in, (h, c))            # out: [B,1,H]\n",
    "        logits = self.out(out.squeeze(1))                        # [B,V]\n",
    "        return logits, (h2, c2), attn\n",
    "\n",
    "    def forward(self, src_ids, tgt_in, mask):\n",
    "        # Teacher forcing full pass → logits [B,T,V]\n",
    "        enc_out, state = self.encode(src_ids)\n",
    "        T = tgt_in.size(1)\n",
    "        logits_list = []\n",
    "        for t in range(T):\n",
    "            y_prev = tgt_in[:, t:t+1]            # [B,1]\n",
    "            logits, state, _ = self.decode_step(y_prev, state, enc_out, mask)\n",
    "            logits_list.append(logits.unsqueeze(1))\n",
    "        return torch.cat(logits_list, dim=1)      # [B,T,V]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, src_text, tok: DualTokenizer, max_len=64, device='cpu'):\n",
    "        self.eval()\n",
    "        src_ids = tok.source.encode_with_special(src_text, max_len, add_sos=False, add_eos=True)\n",
    "        mask = [1 if t != PAD else 0 for t in src_ids]\n",
    "        src = torch.tensor([src_ids], dtype=torch.long, device=device)\n",
    "        msk = torch.tensor([mask],    dtype=torch.bool, device=device)\n",
    "        enc_out, state = self.encode(src)\n",
    "        cur = torch.tensor([[SOS]], dtype=torch.long, device=device)\n",
    "        outs = []\n",
    "        for _ in range(max_len):\n",
    "            logits, state, _ = self.decode_step(cur, state, enc_out, msk)\n",
    "            cur = torch.argmax(logits, dim=-1, keepdim=True)  # greedy\n",
    "            token = cur.item()\n",
    "            if token == EOS:\n",
    "                break\n",
    "            outs.append(token)\n",
    "        return tok.target.decode(outs)\n",
    "\n",
    "emb_dim   = 256\n",
    "hidden_dim= 512\n",
    "num_layers= 2\n",
    "dropout   = 0.3\n",
    "\n",
    "model = AttentionRNN(srcV, tgtV, emb_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 5) Training Utilities (Loss, Loop, Eval)\n",
    "def train_one_epoch(model, dl, optimizer, pad_id=PAD, max_norm=1.0):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    n = 0\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "    for batch in dl:\n",
    "        src, tgt_in, tgt_out, mask = [x.to(device) for x in batch]\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(src, tgt_in, mask)            # [B,T,V]\n",
    "        B,T,V = logits.shape\n",
    "        loss = criterion(logits.view(B*T, V), tgt_out.view(B*T))\n",
    "        loss.backward()\n",
    "        if max_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        optimizer.step()\n",
    "        total += loss.item()\n",
    "        n += 1\n",
    "    return total / max(n,1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dl, pad_id=PAD):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    n = 0\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "    for batch in dl:\n",
    "        src, tgt_in, tgt_out, mask = [x.to(device) for x in batch]\n",
    "        logits = model(src, tgt_in, mask)            # [B,T,V]\n",
    "        B,T,V = logits.shape\n",
    "        loss = criterion(logits.view(B*T, V), tgt_out.view(B*T))\n",
    "        total += loss.item()\n",
    "        n += 1\n",
    "    return total / max(n,1)\n",
    "\n",
    "def bleu_score_simple(ref: str, hyp: str, max_n=4):\n",
    "    # BLEU sederhana (tanpa brevity penalty rumit – cukup indikatif)\n",
    "    def ngrams(tokens, n):\n",
    "        return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "    ref_toks = ref.split()\n",
    "    hyp_toks = hyp.split()\n",
    "    precisions = []\n",
    "    for n in range(1, max_n+1):\n",
    "        ref_ngrams = Counter(ngrams(ref_toks, n))\n",
    "        hyp_ngrams = Counter(ngrams(hyp_toks, n))\n",
    "        overlap = sum(min(count, ref_ngrams[ng]) for ng, count in hyp_ngrams.items())\n",
    "        total = max(sum(hyp_ngrams.values()), 1)\n",
    "        precisions.append(overlap / total)\n",
    "    # geometric mean\n",
    "    score = 1.0\n",
    "    for p in precisions:\n",
    "        score *= max(p, 1e-9)\n",
    "    score = score ** (1/len(precisions))\n",
    "    # brevity penalty (sederhana)\n",
    "    bp = math.exp(min(0, 1 - len(ref_toks) / max(len(hyp_toks),1)))\n",
    "    return bp * score\n",
    "\n",
    "def rouge1(ref: str, hyp: str):\n",
    "    ref_t = ref.split(); hyp_t = hyp.split()\n",
    "    ref_c = Counter(ref_t); hyp_c = Counter(hyp_t)\n",
    "    overlap = sum(min(ref_c[w], hyp_c[w]) for w in ref_c)\n",
    "    P = overlap / max(len(hyp_t), 1)\n",
    "    R = overlap / max(len(ref_t), 1)\n",
    "    F1 = 2*P*R / max(P+R, 1e-9)\n",
    "    return P, R, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train 5.1178 | val 5.0211 | 51.0s\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 6) Train!\n",
    "EPOCHS = 1  # naikkan sesuai kebutuhan\n",
    "LR = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    tr_loss = train_one_epoch(model, train_dl, optimizer)\n",
    "    va_loss = evaluate(model, val_dl)\n",
    "    dt = time.time()-t0\n",
    "    print(f\"Epoch {epoch:02d} | train {tr_loss:.4f} | val {va_loss:.4f} | {dt:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "Source   : word\n",
      "Reference: kata\n",
      "Generated: \n",
      "BLEU≈0.0000 | ROUGE-1 P=0.000 R=0.000 F1=0.000\n",
      "\n",
      "---\n",
      "Source   : sometimes, occasionally\n",
      "Reference: kadang-kadang, terkadang\n",
      "Generated: \n",
      "BLEU≈0.0000 | ROUGE-1 P=0.000 R=0.000 F1=0.000\n",
      "\n",
      "---\n",
      "Source   : use | to use\n",
      "Reference: kgunaan; mengunakan\n",
      "Generated: \n",
      "BLEU≈0.0000 | ROUGE-1 P=0.000 R=0.000 F1=0.000\n",
      "\n",
      "---\n",
      "Source   : but, still\n",
      "Reference: tetapi, masih\n",
      "Generated: \n",
      "BLEU≈0.0000 | ROUGE-1 P=0.000 R=0.000 F1=0.000\n",
      "\n",
      "---\n",
      "Source   : fish\n",
      "Reference: ikan\n",
      "Generated: \n",
      "BLEU≈0.0000 | ROUGE-1 P=0.000 R=0.000 F1=0.000\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 7) Quick test: Greedy decoding + BLEU & ROUGE-1\n",
    "model.eval()\n",
    "SAMPLES = min(len(val_df), 5)\n",
    "for i in random.sample(range(len(val_df)), SAMPLES):\n",
    "    src = val_df.loc[i, 'source']\n",
    "    ref = val_df.loc[i, 'target']\n",
    "    hyp = model.generate(src, dual_tok, max_len=MAX_TGT_LEN, device=device)\n",
    "    print(\"\\n---\")\n",
    "    print(\"Source   :\", src)\n",
    "    print(\"Reference:\", ref)\n",
    "    print(\"Generated:\", hyp)\n",
    "    bleu = bleu_score_simple(ref, hyp)\n",
    "    p,r,f1 = rouge1(ref, hyp)\n",
    "    print(f\"BLEU≈{bleu:.4f} | ROUGE-1 P={p:.3f} R={r:.3f} F1={f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to artifacts/\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 8) Save model & tokenizer\n",
    "os.makedirs('artifacts', exist_ok=True)\n",
    "torch.save(model.state_dict(), 'artifacts/mt_attention_rnn.pt')\n",
    "\n",
    "import json\n",
    "tok_art = {\n",
    "    'src_stoi': dual_tok.source.stoi,\n",
    "    'tgt_stoi': dual_tok.target.stoi,\n",
    "    'special': {'PAD': PAD, 'SOS': SOS, 'EOS': EOS, 'UNK': UNK}\n",
    "}\n",
    "with open('artifacts/tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(tok_art, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved to artifacts/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
