{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d58f3de",
   "metadata": {},
   "source": [
    "# Transformer Text Generation (PyTorch)\n",
    "\n",
    "Implementasi Transformer untuk text generation. Notebook ini dirancang agar metrik evaluasi konsisten dengan implementasi Rust (metrics.rs): BLEU dengan brevity penalty dan smoothing epsilon, diversity n-gram, serta repetition rate berbasis trigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6f8d6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, math, random\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3617f5f5",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "334b7fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, max_vocab_size=10000):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        # special tokens\n",
    "        self.PAD_TOKEN = '<PAD>'\n",
    "        self.UNK_TOKEN = '<UNK>'\n",
    "        self.SOS_TOKEN = '<SOS>'\n",
    "        self.EOS_TOKEN = '<EOS>'\n",
    "        self.word2idx[self.PAD_TOKEN] = 0\n",
    "        self.word2idx[self.UNK_TOKEN] = 1\n",
    "        self.word2idx[self.SOS_TOKEN] = 2\n",
    "        self.word2idx[self.EOS_TOKEN] = 3\n",
    "        for idx, tok in enumerate([self.PAD_TOKEN, self.UNK_TOKEN, self.SOS_TOKEN, self.EOS_TOKEN]):\n",
    "            self.idx2word[idx] = tok\n",
    "\n",
    "    def fit(self, texts: List[str]):\n",
    "        counts = Counter()\n",
    "        for t in texts:\n",
    "            counts.update(t.lower().split())\n",
    "        most = counts.most_common(self.max_vocab_size - 4)\n",
    "        for w, _ in most:\n",
    "            if w not in self.word2idx:\n",
    "                idx = len(self.word2idx)\n",
    "                self.word2idx[w] = idx\n",
    "                self.idx2word[idx] = w\n",
    "        print('Vocab size:', len(self.word2idx))\n",
    "\n",
    "    def encode(self, text: str, max_len: int = None, add_sos=False, add_eos=False) -> List[int]:\n",
    "        toks = text.lower().split()\n",
    "        ids = []\n",
    "        if add_sos:\n",
    "            ids.append(self.word2idx[self.SOS_TOKEN])\n",
    "        ids.extend(self.word2idx.get(w, self.word2idx[self.UNK_TOKEN]) for w in toks)\n",
    "        if add_eos:\n",
    "            ids.append(self.word2idx[self.EOS_TOKEN])\n",
    "        if max_len is not None:\n",
    "            if len(ids) > max_len:\n",
    "                ids = ids[:max_len]\n",
    "            else:\n",
    "                ids += [self.word2idx[self.PAD_TOKEN]] * (max_len - len(ids))\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        out = []\n",
    "        for i in ids:\n",
    "            w = self.idx2word.get(int(i), self.UNK_TOKEN)\n",
    "            if w in (self.PAD_TOKEN, self.SOS_TOKEN, self.EOS_TOKEN):\n",
    "                continue\n",
    "            out.append(w)\n",
    "        return ' '.join(out)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    @property\n",
    "    def pad_token_id(self) -> int:\n",
    "        return self.word2idx[self.PAD_TOKEN]\n",
    "\n",
    "    @property\n",
    "    def eos_token_id(self) -> int:\n",
    "        return self.word2idx[self.EOS_TOKEN]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8616a7a",
   "metadata": {},
   "source": [
    "## Dataset dan Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "628b5aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lines(path: str, min_words=3) -> List[str]:\n",
    "    if not os.path.exists(path):\n",
    "        return []\n",
    "    out = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            t = line.strip().strip('\"').lower()\n",
    "            if len(t.split()) >= min_words:\n",
    "                out.append(t)\n",
    "    return out\n",
    "\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], tokenizer: Tokenizer, max_seq_len: int = 128):\n",
    "        self.texts = texts\n",
    "        self.tok = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        t = self.texts[idx]\n",
    "        ids = self.tok.encode(t, max_len=self.max_seq_len, add_sos=True, add_eos=True)\n",
    "        x = torch.tensor(ids[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(ids[1:],  dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c7e3f9",
   "metadata": {},
   "source": [
    "## Model: Transformer LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f450926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.q = nn.Linear(d_model, d_model)\n",
    "        self.k = nn.Linear(d_model, d_model)\n",
    "        self.v = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "    def forward(self, x, mask=None):\n",
    "        B, S, D = x.size()\n",
    "        Q = self.q(x).view(B, S, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.k(x).view(B, S, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.v(x).view(B, S, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        scores = (Q @ K.transpose(-2, -1)) / self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = attn @ V\n",
    "        out = out.transpose(1, 2).contiguous().view(B, S, D)\n",
    "        return self.out(out)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ff   = FeedForward(d_model, d_ff, dropout)\n",
    "        self.do1 = nn.Dropout(dropout)\n",
    "        self.do2 = nn.Dropout(dropout)\n",
    "    def forward(self, x, mask=None):\n",
    "        h = self.attn(self.ln1(x), mask)\n",
    "        x = x + self.do1(h)\n",
    "        h = self.ff(self.ln2(x))\n",
    "        x = x + self.do2(h)\n",
    "        return x\n",
    "\n",
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, n_heads=8, n_layers=4, d_ff=1024, max_seq_len=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, d_model)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Embedding):\n",
    "                nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "    def causal_mask(self, S, device):\n",
    "        return torch.tril(torch.ones(S, S, device=device)).unsqueeze(0).unsqueeze(0)\n",
    "    def forward(self, x):\n",
    "        B, S = x.shape\n",
    "        device = x.device\n",
    "        pos = torch.arange(0, S, device=device).unsqueeze(0).expand(B, S)\n",
    "        h = self.dropout(self.tok_emb(x) + self.pos_emb(pos))\n",
    "        mask = self.causal_mask(S, device)\n",
    "        for blk in self.blocks:\n",
    "            h = blk(h, mask)\n",
    "        h = self.ln_f(h)\n",
    "        return self.head(h)  # [B, S, V]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca063993",
   "metadata": {},
   "source": [
    "## Metrics (selaras metrics.rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d454cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(loss: float) -> float:\n",
    "    try:\n",
    "        return math.exp(loss)\n",
    "    except OverflowError:\n",
    "        return float('inf')\n",
    "\n",
    "def _ngram_counts(tokens: List[str], n: int) -> Counter:\n",
    "    c = Counter()\n",
    "    L = len(tokens)\n",
    "    if L < n:\n",
    "        return c\n",
    "    for i in range(L - n + 1):\n",
    "        c[' '.join(tokens[i:i+n])] += 1\n",
    "    return c\n",
    "\n",
    "def bleu_bp_smooth(reference: str, candidate: str, max_n: int = 4, eps: float = 1e-9):\n",
    "    ref = reference.split()\n",
    "    cand = candidate.split()\n",
    "    if not ref or not cand:\n",
    "        return 0.0, [0.0]*max_n, 0.0\n",
    "    precisions = []\n",
    "    for n in range(1, max_n+1):\n",
    "        ref_c  = _ngram_counts(ref, n)\n",
    "        cand_c = _ngram_counts(cand, n)\n",
    "        cand_total = sum(cand_c.values())\n",
    "        if cand_total == 0:\n",
    "            precisions.append(0.0)\n",
    "            continue\n",
    "        matched = sum(min(cnt, ref_c.get(ng, 0)) for ng, cnt in cand_c.items())\n",
    "        p_n = (matched + eps) / (cand_total + eps)\n",
    "        precisions.append(p_n)\n",
    "    c, r = float(len(cand)), float(len(ref))\n",
    "    bp = math.exp(1.0 - r/c) if c < r else 1.0\n",
    "    if any(p <= 0.0 for p in precisions):\n",
    "        geo = 0.0\n",
    "    else:\n",
    "        geo = math.exp(sum(math.log(p) for p in precisions) / max_n)\n",
    "    return bp * geo, precisions, bp\n",
    "\n",
    "def bleu_n_with_bp(reference: str, candidate: str, n: int = 2, eps: float = 1e-9) -> float:\n",
    "    bleu, _, _ = bleu_bp_smooth(reference, candidate, max_n=n, eps=eps)\n",
    "    return bleu\n",
    "\n",
    "def bleu_score(reference: str, candidate: str, n: int = 1) -> float:\n",
    "    ref = reference.split()\n",
    "    cand = candidate.split()\n",
    "    if len(cand) == 0 or len(ref) < n:\n",
    "        return 0.0\n",
    "    ref_c  = _ngram_counts(ref, n)\n",
    "    cand_c = _ngram_counts(cand, n)\n",
    "    matched = sum(min(cnt, ref_c.get(ng, 0)) for ng, cnt in cand_c.items())\n",
    "    total   = sum(cand_c.values())\n",
    "    return matched / max(1, total)\n",
    "\n",
    "def diversity_score(text: str, n: int = 2) -> float:\n",
    "    toks = text.split()\n",
    "    L = len(toks)\n",
    "    if L < n:\n",
    "        return 0.0\n",
    "    total = L - n + 1\n",
    "    seen = set(' '.join(toks[i:i+n]) for i in range(total))\n",
    "    return len(seen) / float(total)\n",
    "\n",
    "def repetition_score(text: str) -> float:\n",
    "    toks = text.split()\n",
    "    L = len(toks)\n",
    "    if L < 3:\n",
    "        return 0.0\n",
    "    cnt = Counter(' '.join(toks[i:i+3]) for i in range(L-2))\n",
    "    repeated = sum(v for v in cnt.values() if v > 1)\n",
    "    return repeated / float(L - 2)\n",
    "\n",
    "class GenerationMetrics:\n",
    "    def __init__(self, perplexity, bleu_1, bleu_2, diversity_2, diversity_3, repetition):\n",
    "        self.perplexity = perplexity\n",
    "        self.bleu_1 = bleu_1\n",
    "        self.bleu_2 = bleu_2\n",
    "        self.diversity_2 = diversity_2\n",
    "        self.diversity_3 = diversity_3\n",
    "        self.repetition = repetition\n",
    "    @classmethod\n",
    "    def calculate(cls, loss: float, reference: str | None, candidate: str):\n",
    "        ppl = perplexity(loss)\n",
    "        if reference is not None:\n",
    "            b1 = bleu_n_with_bp(reference, candidate, n=1)\n",
    "            b2 = bleu_n_with_bp(reference, candidate, n=2)\n",
    "        else:\n",
    "            b1 = 0.0\n",
    "            b2 = 0.0\n",
    "        d2 = diversity_score(candidate, 2)\n",
    "        d3 = diversity_score(candidate, 3)\n",
    "        rep = repetition_score(candidate)\n",
    "        return cls(ppl, b1, b2, d2, d3, rep)\n",
    "\n",
    "def self_bleu(texts: List[str], n: int = 2) -> float:\n",
    "    if len(texts) < 2:\n",
    "        return 0.0\n",
    "    s, c = 0.0, 0\n",
    "    for i in range(len(texts)):\n",
    "        for j in range(len(texts)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            s += bleu_n_with_bp(texts[i], texts[j], n=n)\n",
    "            c += 1\n",
    "    return s / c if c > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e676cc",
   "metadata": {},
   "source": [
    "## Top-k filtering dan generator wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfbb7d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_filtering(logits, top_k=50):\n",
    "    if top_k is None or top_k <= 0:\n",
    "        return logits\n",
    "    k = min(top_k, logits.size(-1))\n",
    "    topk = torch.topk(logits, k=k, dim=-1)\n",
    "    thresh = topk.values[..., -1].unsqueeze(-1)\n",
    "    return torch.where(logits < thresh, torch.tensor(float('-inf'), device=logits.device), logits)\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_from_prompt(model: TransformerLM, tok: Tokenizer, prompt: str, max_new=50, temperature=0.9, top_k=50, seq_len=128, eos_id=None):\n",
    "    start_ids = tok.encode(prompt, max_len=64, add_sos=True, add_eos=False)\n",
    "    tokens = torch.tensor(start_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    model.eval()\n",
    "    for _ in range(max_new):\n",
    "        if tokens.size(1) >= seq_len:\n",
    "            break\n",
    "        logits = model(tokens)\n",
    "        last = logits[:, -1, :] / max(1e-8, temperature)\n",
    "        last = top_k_filtering(last, top_k=top_k)\n",
    "        probs = F.softmax(last, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        tokens = torch.cat([tokens, next_id], dim=1)\n",
    "        if eos_id is not None and int(next_id.item()) == eos_id:\n",
    "            break\n",
    "    return tok.decode(tokens[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5252c021",
   "metadata": {},
   "source": [
    "## Data preparation dan hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9b2b25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 10\n",
      "Train: 9 | Val: 1\n",
      "Vocab size: 69\n",
      "Total parameters: 3.23M\n"
     ]
    }
   ],
   "source": [
    "CORPUS = 'TextGen1.txt'\n",
    "texts = load_lines(CORPUS, min_words=3)\n",
    "if len(texts) == 0:\n",
    "    demo = [\n",
    "        'so shaken as we are so wan with care',\n",
    "        'find we a time for frighted peace to pant',\n",
    "        'and breathe short winded accents of new broils',\n",
    "        'to be commenced in strands afar remote',\n",
    "        'no more the thirsty entrance of this soil',\n",
    "        'shall daub her lips with her own childrens blood'\n",
    "    ]\n",
    "    with open('demo_text.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(demo))\n",
    "    texts = demo\n",
    "print('Total lines:', len(texts))\n",
    "\n",
    "random.shuffle(texts)\n",
    "split = max(1, int(0.9 * len(texts)))\n",
    "train_texts = texts[:split]\n",
    "val_texts   = texts[split:]\n",
    "print(f'Train: {len(train_texts)} | Val: {len(val_texts)}')\n",
    "\n",
    "tok = Tokenizer(max_vocab_size=10000)\n",
    "tok.fit(train_texts + val_texts)\n",
    "PAD = tok.pad_token_id\n",
    "EOS = tok.eos_token_id\n",
    "\n",
    "D_MODEL, N_HEADS, N_LAYERS = 256, 8, 4\n",
    "D_FF, DROPOUT = 1024, 0.1\n",
    "SEQ_LEN, BATCH, EPOCHS, LR = 128, 32, 1, 1e-4\n",
    "\n",
    "train_ds = LMDataset(train_texts, tok, SEQ_LEN)\n",
    "val_ds   = LMDataset(val_texts, tok, SEQ_LEN)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True, drop_last=True)\n",
    "val_dl   = DataLoader(val_ds, batch_size=BATCH, shuffle=False, drop_last=False)\n",
    "\n",
    "model = TransformerLM(tok.vocab_size, D_MODEL, N_HEADS, N_LAYERS, D_FF, SEQ_LEN, DROPOUT).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "print(f'Total parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f061ca41",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d112c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train 0.0000 (ppl 1.00) | val 4.3206 (ppl 75.24)\n",
      "Saved -> artifacts/best_transformer.pt\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, opt, device, pad_id):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=pad_id)\n",
    "        opt.zero_grad(); loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        total += float(loss.item())\n",
    "    return total / max(1, len(loader))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, pad_id):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=pad_id)\n",
    "        total += float(loss.item())\n",
    "    return total / max(1, len(loader))\n",
    "\n",
    "best_val = float('inf')\n",
    "for e in range(1, EPOCHS+1):\n",
    "    tr = train_epoch(model, train_dl, opt, device, PAD)\n",
    "    va = evaluate(model, val_dl, device, PAD)\n",
    "    print(f'Epoch {e} | train {tr:.4f} (ppl {perplexity(tr):.2f}) | val {va:.4f} (ppl {perplexity(va):.2f})')\n",
    "    if va < best_val:\n",
    "        best_val = va\n",
    "        os.makedirs('artifacts', exist_ok=True)\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'vocab': tok.word2idx,\n",
    "            'cfg': {'d_model': D_MODEL, 'n_heads': N_HEADS, 'n_layers': N_LAYERS, 'd_ff': D_FF, 'dropout': DROPOUT, 'seq_len': SEQ_LEN}\n",
    "        }, 'artifacts/best_transformer.pt')\n",
    "        print('Saved -> artifacts/best_transformer.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a82486",
   "metadata": {},
   "source": [
    "## Evaluasi dan contoh generasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade8fe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing text generation...\n",
      "============================================================\n",
      "\n",
      " Generation Examples:\n",
      "\n",
      "1. Prompt: \"once upon a time\"\n",
      "   [T=0.7] <UNK> <UNK> a <UNK> a his bold going mirror, his hath hath whilst-- timon's till friends pike,\n",
      "   [T=1.0] <UNK> <UNK> a <UNK> knife, engine, engine, noble give your wealth, pike, going them.\n",
      "   [T=1.2] <UNK> <UNK> a <UNK> ionia, lydia strong noble give in engine, myself in bold timon's a myself hath which which him or death sword, aspiring timon's my his should them. friends need do you. you. death hand my banqueting aspiring i them. seem'd lord ionia, mutinous whilst-- death going a lord sword, sword, aspiring ionia, i need in <UNK> which need shall myself\n",
      "\n",
      "2. Prompt: \"the quick brown\"\n",
      "   [T=0.7] <UNK> <UNK> <UNK> his whilst-- seem'd should friends do engine, a aspiring friends which mutinous engine, his which myself profess strength them. in wealth, <UNK> not rider rider them. you. deliver like for\n",
      "   [T=1.0] <UNK> <UNK> <UNK> his i hath knife, strong do hath lydia engine, myself a a\n",
      "   [T=1.2] <UNK> <UNK> <UNK> profess mutinous should them. be rider like death engine, lydia hath profess rider deliver sword, engine, that strong aspiring wealth, <UNK> need <UNK> going lydia\n",
      "\n",
      "3. Prompt: \"in the beginning\"\n",
      "   [T=0.7] in <UNK> <UNK> whilst-- mirror, wealth, engine, hath lydia deliver mutinous lydia need\n",
      "   [T=1.0] in <UNK> <UNK> his or yet mutinous rider aspiring a strong a i which friends a lord need friends <UNK> or should that strength any your mirror, any aspiring should noble in <UNK> seem'd i in wealth, give engine, i seem'd should knife, mirror, shall knife, his strong engine, his his going that friends deliver that pike, spent myself deliver knife,\n",
      "   [T=1.2] in <UNK> <UNK> engine, need death strength help be them. friends strong yet lydia timon's death i lord should any rider that <UNK> peace, should till deliver bold his lydia going shall i mutinous like mutinous war, shall them. strength like which should wealth, spent\n",
      "\n",
      "\n",
      " Validation Samples:\n",
      "\n",
      "1. Original:\n",
      "   till your strong hand shall help to give him strength\n",
      "   Generated:\n",
      "   till your strong them. deliver myself rider spent myself remember'd that pike, death my noble that ionia, give hath lydia his mirror, lord them. i profess which give myself which should spent going a myself friends whilst-- pike, his should i shall knife, do lack noble do ionia, yet your death going engine, going lydia friends whilst-- you.\n",
      "\n",
      "   Metrics:\n",
      "     BLEU-1: 0.086\n",
      "     BLEU-2: 0.055\n",
      "     Diversity-2: 0.982\n",
      "     Repetition: 0.000\n",
      "--------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "Overall Diversity (Self-BLEU-2): 0.000\n",
      "  (Lower is more diverse)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "T_LIST = [0.7, 1.0, 1.2]\n",
    "USE_TOP_K = True     # set ke False kalau mau lihat efek suhu lebih kentara\n",
    "TOP_K_VAL = 50       # diabaikan kalau USE_TOP_K=False\n",
    "\n",
    "print(\"\\nTesting text generation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============= 1. GENERATION WITH DIFFERENT TEMPERATURES =============\n",
    "print(\"\\n 1. Generation Examples (Different Temperatures):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "prompts = [\"once upon a time\", \"the quick brown\", \"in the beginning\"]\n",
    "for i, p in enumerate(prompts, 1):\n",
    "    print(f'\\n{i}. Prompt: \"{p}\"')\n",
    "    for T in T_LIST:\n",
    "        out = generate_from_prompt(\n",
    "            model, tok, p,\n",
    "            max_new=50,  # 50 seperti di Rust\n",
    "            temperature=T,\n",
    "            top_k=(TOP_K_VAL if USE_TOP_K else None),\n",
    "            seq_len=SEQ_LEN,\n",
    "            eos_id=EOS\n",
    "        )\n",
    "        print(f\"   [T={T:.1f}] {out}\")\n",
    "\n",
    "# ============= 2. VALIDATION SAMPLES COMPARISON =============\n",
    "print(\"\\n\\n 2. Validation Samples Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if len(val_texts) == 0:\n",
    "    print(\"  (no validation samples)\")\n",
    "else:\n",
    "    k = min(3, len(val_texts))\n",
    "    samples = random.sample(val_texts, k=k)\n",
    "\n",
    "    for idx, ref in enumerate(samples, 1):\n",
    "        print(f\"\\n{idx}. Original:\")\n",
    "        preview = ref[:100] + (\"...\" if len(ref) > 100 else \"\")\n",
    "        print(f\"   {preview}\")\n",
    "\n",
    "        # prompt = 3 kata pertama dari reference\n",
    "        pr = \" \".join(ref.split()[:3])\n",
    "\n",
    "        gen = generate_from_prompt(\n",
    "            model, tok, pr,\n",
    "            max_new=60,\n",
    "            temperature=0.8,\n",
    "            top_k=(TOP_K_VAL if USE_TOP_K else None),\n",
    "            seq_len=SEQ_LEN,\n",
    "            eos_id=EOS\n",
    "        )\n",
    "        print(\"   Generated:\")\n",
    "        print(f\"   {gen}\")\n",
    "\n",
    "        # hitung metrik\n",
    "        m = GenerationMetrics.calculate(loss=0.0, reference=ref, candidate=gen)\n",
    "        print(\"   Metrics:\")\n",
    "        print(f\"     BLEU-1: {m.bleu_1:.3f}\")\n",
    "        print(f\"     BLEU-2: {m.bleu_2:.3f}\")\n",
    "        print(f\"     Diversity-2: {m.diversity_2:.3f}\")\n",
    "        print(f\"     Repetition: {m.repetition:.3f}\")\n",
    "\n",
    "# ============= 3. MODEL DIVERSITY TEST (Self-BLEU) =============\n",
    "print(\"\\n\\n 3. Model Diversity Test (Self-BLEU):\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Testing how diverse the model's outputs are when given the same prompt.\")\n",
    "print(\"Lower Self-BLEU = More diverse outputs (better creativity)\")\n",
    "print()\n",
    "\n",
    "diversity_prompts = [\"once upon a time\", \"the quick brown\", \"in the beginning\"]\n",
    "num_generations = 8  # Generate 8 teks per prompt\n",
    "\n",
    "for idx, prompt in enumerate(diversity_prompts, 1):\n",
    "    print(f\"\\n{idx}.{'─' * 55} Prompt: \\\"{prompt}\\\"\")\n",
    "    \n",
    "    generated_texts = []\n",
    "    \n",
    "    # Generate beberapa kali dengan prompt yang SAMA\n",
    "    for i in range(num_generations):\n",
    "        gen = generate_from_prompt(\n",
    "            model, tok, prompt,\n",
    "            max_new=60,\n",
    "            temperature=0.8,  # temperature tetap\n",
    "            top_k=(TOP_K_VAL if USE_TOP_K else None),\n",
    "            seq_len=SEQ_LEN,\n",
    "            eos_id=EOS\n",
    "        )\n",
    "        \n",
    "        # Print preview\n",
    "        preview = gen[:70] + \"...\" if len(gen) > 70 else gen\n",
    "        print(f\"   {i + 1}. {preview}\")\n",
    "        \n",
    "        generated_texts.append(gen)\n",
    "    \n",
    "    # Hitung Self-BLEU untuk prompt ini\n",
    "    self_bleu_2 = self_bleu(generated_texts, 2)\n",
    "    self_bleu_3 = self_bleu(generated_texts, 3)\n",
    "    \n",
    "    print(f\"\\n   Diversity Metrics for this prompt:\")\n",
    "    print(f\"     Self-BLEU-2: {self_bleu_2:.3f} (bigram overlap)\")\n",
    "    print(f\"     Self-BLEU-3: {self_bleu_3:.3f} (trigram overlap)\")\n",
    "    \n",
    "    # Interpretasi\n",
    "    if self_bleu_2 < 0.3:\n",
    "        interpretation = \"Excellent diversity!\"\n",
    "    elif self_bleu_2 < 0.5:\n",
    "        interpretation = \"Good diversity\"\n",
    "    elif self_bleu_2 < 0.7:\n",
    "        interpretation = \"Moderate diversity\"\n",
    "    else:\n",
    "        interpretation = \"Low diversity (repetitive outputs)\"\n",
    "    \n",
    "    print(f\"     Interpretation: {interpretation}\")\n",
    "\n",
    "# ============= 4. TEMPERATURE IMPACT ON DIVERSITY =============\n",
    "print(\"\\n\\n 4. Temperature Impact on Diversity:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "test_prompt = \"once upon a time\"\n",
    "temperatures = [0.5, 0.8, 1.0, 1.2]\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n   Temperature: {temp:.1f}\")\n",
    "    generated_texts = []\n",
    "    \n",
    "    for i in range(6):\n",
    "        gen = generate_from_prompt(\n",
    "            model, tok, test_prompt,\n",
    "            max_new=60,\n",
    "            temperature=temp,\n",
    "            top_k=(TOP_K_VAL if USE_TOP_K else None),\n",
    "            seq_len=SEQ_LEN,\n",
    "            eos_id=EOS\n",
    "        )\n",
    "        \n",
    "        preview = gen[:60] + \"...\" if len(gen) > 60 else gen\n",
    "        print(f\"     {i + 1}. {preview}\")\n",
    "        \n",
    "        generated_texts.append(gen)\n",
    "    \n",
    "    sb = self_bleu(generated_texts, 2)\n",
    "    print(f\"     → Self-BLEU-2: {sb:.3f}\")\n",
    "\n",
    "# ============= 5. SUMMARY =============\n",
    "print(\"\\n\\n\" + \"=\" * 60)\n",
    "print(\" SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"✓ Generation test completed\")\n",
    "print(\"✓ Lower Self-BLEU indicates more diverse and creative outputs\")\n",
    "print(\"✓ Higher temperature generally increases diversity\")\n",
    "print(\"✓ Optimal temperature balances diversity and coherence (0.7-1.0)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d56631",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
